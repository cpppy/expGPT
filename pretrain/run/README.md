

1. train_llama410M
just train with single GPU
   
2. train_llama410M_v2
+ fp16

3. train_llama410M_v3
+ ddp

model structure from lit-llama
dataset from lit-llama
train_proc from nanoGPT


# TODO List
flash attention
deepspeed

instruct training
RLHF





















